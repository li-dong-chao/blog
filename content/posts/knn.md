---
title: k近邻
date: 2023-10-05
categories: ["机器学习"]
---


k近邻算法是一种简单、直观的分类算法（也可用于回归，但比较少），其核心思想是：
对于一个给定的训练集，假设现在需要预测一个新样本的类别，k近邻算法会在训练集中找到距离新样本最近的k个样本，
然后如果这k个点多数属于某个类别，就认为这个点也属于这个类别。

## k近邻算法

下面详细说明一下k近邻的算法过程。

 **输入**：训练集 $T = \{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$ ，
 其中 $y_i \in \mathcal{Y} = \{c_1, c_2, c_K\}$ 表示类别；待分类的样本 $x$ 。

 **输出**：待分类样本 $x$ 的类别。

 1. 根据实际数据情况，确定要使用的距离度量；
 2. 在训练集 $T$ 中找到距离 $x$ 最近的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的邻域记作 $N_k(x)$ ；
 3. 在 $N_k(x)$ 中根据分类决策规则（如多数表决）决定 $x$ 的类别 $y$ ：
    
    $$
    y = \arg\max_{c_j} \sum_{x_i \in N_k(x)} I(y_i=c_j), i=1,2,\dots,N; j=1,2,\dots,K
    $$

    其中，$I$ 表示指示函数，即当 $y_i=c_j$ 成立时 $I=1$ ，否则 $I=0$ 。

分析上面的算法过程，可以发现K近邻算法主要由以下三个点决定：

* k值大小
* 距离度量标准
* 分类决策规则

这三点被称为K近邻算法的三个基本要素，对于一个给定的训练集，如果这三个元素确定，那么 $x$ 的分类结果是唯一确定的。

> 当k值取1时，k近邻算法收敛为最近邻算法。最近邻算法将训练集中距离 $x$ 点最近点的类别作为 $x$ 的类别。

## k近邻模型

### k近邻模型的几何理解

从几何角度出发，k近邻模型本质上表示一种空间划分形式，该划分将样本所在的特征空间划分为了若干个区域，
每个区域属于某一个的类别。可参考下图进行理解。

![Snipaste_2023-04-25_13-13-05](https://littletom.oss-cn-nanjing.aliyuncs.com/Snipaste_2023-04-25_13-13-05.png)

一旦训练集和模型三要素确定，那么空间划分便唯一确定。

### 距离

k近邻算法使用两个样本点之间的距离来度量两个点的相似程度。
距离度量的选择由很多，比较常用的有欧式距离、闵可夫斯基（Minkowski）距离等。
下面介绍一些距离的定义。

* $L_p$ 距离
  
  对于 $n$ 维特征空间中的两个样本 $x_i=(x_i^{(1)}, x_i^{(2)}, \dots, x_i^{(n)})^T$ 和 $x_j=(x_j^{(1)}, x_j^{(2)}, \dots, x_j^{(n)})^T$ ，
  两者之间的 $L_p$ 距离为：

$$
L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)}|^p)^{\frac{1}{p}}
$$

  其中 $p \ge 1$ 。

* 欧式距离

  当 $p=2$ 时，$L_p$ 距离收缩为欧氏距离。

* 曼哈顿距离
  
  当 $p=1$ 时，$L_p$ 距离收缩为曼哈顿距离。

> 选用不同的距离独立标准，得到的紧邻点可能是不同的。
>

### k值

k值的选择会对k近邻算法的结果产生较大的影响。

当k值选的比较小时，模型预测所用的邻域比较小，模型复杂度变高，容易过拟合，
预测结果的近似误差会减小，但估计误差会增大。

当k值选的比较大时，模型预测所用的邻域比较大，模型复杂度变低，容易欠拟合，
预测结果的估计误差会减小，但近似误差会增大。

在实际应用过程中，往往会选择一个相对较小的k值，且一般会通过交叉验证法来选择出一个最优的k值。

### 分类决策规则

k近邻算法一般使用多数表决策略作为分类决策规则。
之所以使用该策略，是因为多数表决是经验风险最小化的一种实现。
