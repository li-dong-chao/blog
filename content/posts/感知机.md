---
title: 感知机
date: 2023-10-05
categories: ["机器学习"]
tags: ["算法"]
series: ["算法学习笔记"]
---

感知机模型是一种线性二分类判别分类模型，它根据输入的样本的特征向量，输出样本的类型。

感知机模型假设输入数据是线性可分，其学习的目标为找到一个线性分离超平面，从而实现对样本的正确分类。

感知机具有简单、基于实现的优点，分为原始形式和对偶形式，是神经网络和支持向量机的基础。

## 感知机模型

假设输入空间为 $\mathcal{X} \subseteq R^n$ ，输出空间为 $\mathcal{y} = {+1, -1}$ 。
输入 $x \in \mathcal{X}$ 表示样本的特征向量，表示输入空间中的样本点。
输出 $y \in \mathcal{Y}$ 表示样本的类别。
则感知机可用下面的函数表示：

$$
f(x) = sign(w \cdot x + b)
$$

其中，$w$ 和 $b$ 是感知机模型的参数，
$w \in R^n$ 称为权值，在一定程度上反映了各个特征向量的重要程度，
$b \in R$ 称为偏置。
公式中的点表示内积，sign为符号函数，即

$$
sign(x) = \begin{cases}
            +1,& x \ge 0\\\
            -1,& x \lt 0
            \end{cases}
$$

感知机模型的假设空间是定义在特征空间中的所有线性分类模型，
即函数集合 ${f| f(x) = w \cdot x + b}$ 。

## 感知机模型的几何理解

考察线性方程 $w \cdot x + b = 0$ ，
它对应与特征空间中的一个超平面 $S$ ，
其中 $w$ 表示超平面的法向量， $b$ 表示超平面的截距。
在二维空间下，其几何图像如下：

![Snipaste_2023-04-23_13-05-12](https://littletom.oss-cn-nanjing.aliyuncs.com/Snipaste_2023-04-23_13-05-12.png)

感知机模型期望的是寻找到这么一个超平面能够将正负样例（图像中的 `o` 与 `x`）
正确地分离到超平面的两侧。

> 不难看出，超平面是由 $w$ 和 $b$ 决定的。

## 感知机模型的损失函数

关于感知机模型的损失函数，一个直观的想法是将误分类点的数量作为损失函数，
但是它不是参数 $w$ 和 $b$ 的连续可导函数，不方便后续的求解。
更靠谱的选择的是将误分类点到超平面的总距离作为损失函数。

下面介绍损失函数的推导过程。

假设存在一个超平面为 $w \cdot x + b = 0$ ，则对于输入空间中的任意点 $x_0$ ，

它到超平面的距离为：

$$
d = \frac{1}{||w||}|w \cdot x + b |
$$

其中，$||w||$ 表示 $w$ 的 $L_2$ 范数。

对于一个被误分类的数据点 $x_0 = (x_i, y_i)$ ，

当它真实为正样本时（即 $y_i = 1 \gt 0$ ），模型将它误分类为负样本（即 $w \cdot x + b \lt 0$ ），
因此有 $|y_i(w \cdot x_i + b)| = -y_i(w \cdot x + b)$；

当它真实为负样本时（即 $y_i = -1 \lt 0$ ），模型将它误分类为正样本（即 $w \cdot x + b \gt 0$ ），
因此有 $|y_i(w \cdot x_i + b)| = -y_i(w \cdot x + b)$。

由此可以对距离公式进行去绝对值处理。

$$
\begin{aligned}
d &= \frac{1}{||w||} |w \cdot x + b | \\\
  &= -\frac{1}{||w||}y_i(w \cdot x + b )
\end{aligned}
$$

假设存在 $M$ 个误分类点，则所有误分类点到超平面的距离之和为：

$$
D = -\frac{1}{||w||} \sum_{x_i \in M} y_i(w \cdot x + b )
$$

考虑到 $w$ 参数仅决定超平面的方向，其范数大小对超平面的方向不会产生影响，因此可以不考虑 $||w||$ ，
从而得到在给定数据集 $T=\{(x_1, y_1), (x_2, y_2), \dots, (X_N, y_N)\}$ 的条件下，感知机模型的经验风险函数为：

$$
L(w,b) = -\sum_{x_i \in M}y_i(w \cdot x+b)
$$

其中，$M$ 为误分类点的集合。

找到令 $L(w,b)$ 最小的参数 $w$ 和 $b$ ，即求得了感知机模型的解。

## 模型求解

感知机模型采用梯度下降法求解，可以分为原始形式和对偶形式两种。

> 梯度下降法是一种常用的求解机器学习手段，其核心思路是每次都让参数沿当前下降最快的方向（即梯度）下降一定长度，
> 直至损失收敛。
>
> 它的核心步骤是：
>
> 1. 首先给定一组初始参数和学习率参数 $\eta$ ；
> 2. 然后根据数据计算出各个参数对损失函数的梯度；
> 3. 让参数沿梯度方向，下降 $\eta$ 个单位长度；
> 4. 重复2~3，直至模型收敛（收敛条件不同模型可能会有所不同）。
>
> 梯度下降可以分为全量梯度下降、批量梯度下降和随机梯度下降。
> 三者的区别在于计算梯度时使用的样本数量不同：
>
> * 全量梯度下降使用全部的样本来计算梯度；
> * 批量梯度下降将全部样本均为 $k$ 组（批），每次全用一组来计算梯度；
> * 随机梯度下降每次随机选择一个样本来计算梯度。
>
> 三者各有优劣，需要合理选择使用。
>
> * 全量梯度下降求得解的质量比较高，但速度慢；
> * 随机梯度下降求解速度快，但可能陷入局部最优解；
> * 批量梯度下降是两者都考虑的一种权衡方法。
>

### 原始形式

感知机模型采用随机梯度下降法求解模型。

根据损失函数，可以计算得到参数 $w$ 和 $b$ 对损失函数的梯度为：

$$
\begin{aligned}
\nabla_w L(w,b) &= -\sum_{x_i \in M} y_i x_i \\\
\nabla_b l(w,b) &= -\sum_{x_i \in M} y_i
\end{aligned}
$$

由此可以得到感知机模型学习算法的原始形式为：

 **输入**：训练集 $T\{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$，学习率 $\eta (0 \lt \eta \le 1)$ ；

 **输出**：$w, b$ ，感知机模型 $f(x) = sign(wx+b)$

 1. 选取初始参数取值 $w_0$ 和 $b_0$ ；
 2. 在训练集中选取数据 $(x_i, y_i)$  ；
 3. 如果该数据为误分类点（即 $y_i(w*x_i+b) \le 0$），则更新梯度：

    $$
    \begin{aligned}
    w &\gets w + \eta y_i x_i \\\
    b &\gets b + \eta y_i
    \end{aligned}
    $$

 4. 重复2~3，直至训练集中没有误分类点。

### 对偶形式

分析原始形式算法可以发现，$w$ 和 $b$ 会不断地根据样本点取值进行改变，如果取初始值 $w_0=0, b_0=0$，
则有最终模型收敛的 $w$ 和 $b$ 为：

$$
\begin{aligned}
w &= 0 + \alpha_1 y_1 x_1 + \alpha_2 y_2 x_2 + \dots + \alpha_N y_N x_N = \sum_{i=1}^N \alpha_i y_i x_i \\\
b &= 0 + \alpha_1 y_1 + \alpha_2 y_2 + \dots + \alpha_N y_N = \sum_{i=1}^N \alpha_i y_i
\end{aligned}
$$

其中，$\alpha_i = n_i \eta$ ，
$n_i$ 表示在 $w$ 和 $b$ 的更新过程中，共使用样本 $(x_i, y_i)$ 更新了 $n_i$ 次。
当样本 $(x_i, y_i)$ 对应的 $n_i$ 越大时，该样本更新次数越多，
说明该样本越靠近分离超平面，越难分类，其对学习结果的影响越大。

对偶形式的基本想法就是：根据上面的分析过程，将 $w$ 和 $b$ 表示为样本 $(x_i, y_i)$ 的线性组合。

感知机学习算法的对偶形式：

 **输入**：训练集 $T\{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}$，学习率 $\eta (0 \lt \eta \le 1)$ ；

 **输出**：$\alpha, b$ ，感知机模型 $f(x) = sign(\sum_{j=1}^N \alpha_j y_j x_j \cdot x + b)$ ；

 1. 给定初始值 $\alpha \gets 0 , b \gets 0$ ；
 2. 在训练集中选取数据 $(x_i, y_i)$ ；
 3. 如果该数据为误分类点（即 $y_i(\sum_{j=1}^N \alpha_j y_j x_j *x_i+b) \le 0$ ），则更新梯度：

    $$
    \begin{aligned}
    \alpha_j &\gets \alpha_j + \eta \\\
    b &\gets b + \eta y_i
    \end{aligned}
    $$

 4. 重复2~3，直至没有误分类点。

使用对偶形式好处是，涉及训练样本的计算都是以内积形式出现，可以体现将训练样本之间的内积全部计算出来，并以矩阵形式保存，这样模型的训练速度会加快。
这个矩阵就是 Gram 矩阵：

$$
G = [x_i \cdot x_j ]_{N \times N}
$$

> 笔者尝试用python实现了感知机算法，感兴趣的同学可以移步 [github](https://github.com/li-dong-chao/StatisticalLearning/blob/main/algo/perceptron.py) 查看。