---
title: 逻辑回归
date: 2023-10-05
categories: ["机器学习"]
---


逻辑回归是一种经典的分类方法，也是非常常用的对数线性模型，下面对这个模型进行介绍。

## 逻辑斯蒂分布（logistic distinction）

在介绍逻辑回归模型之前，需要先了解一下逻辑斯蒂分布是什么。

### 分布定义

设 $X$ 为连续随机变量，则 $X$ 服从逻辑斯蒂分布是指 $X$ 具有下面的分布函数和概率密度：

$$
F(x)=P(X \leq x) = \frac{1}{1+e^{-(x- \mu)/ \gamma}} \\
f(x)=F'(x)=\frac{e^{-(x- \mu)/ \gamma}}{\gamma (1+e^{-(x- \mu)/ \gamma})^2}
$$

其中， $\mu$ 为位置参数，$\gamma > 0$ 为形状参数。

### 分布图像

该分布的概率密度函数 $f(x)$ 和分布函数 $F(x)$ 的图像大致如下。

todo

### 分布性质

从上面的图像可以明显的看出逻辑斯蒂分布的分布函数 $F(x)$ 的图像是一条 **S** 形的曲线（sigmoid curve），该曲线以点 $(\mu , \frac{1}{2})$ 为中心对称，即有

$$
F(-x+\mu)-\frac{1}{2}=-F(x+\mu)+\frac{1}{2}
$$

同时还可以发现，该函数在中心点的增长速度最快，越远离中心点，增长速度越慢。

另外，形状参数 $\gamma$ 对曲线分布形状的影响是： $\gamma$ 的取值越小，曲线在中心点附近的增长速度越快。

## 二项逻辑回归模型

二项逻辑回归模型是指假设条件概率分布 $P(Y|X)$ 为逻辑斯蒂分布的二分类模型。
其随机变量 $X$ 的取值为实数，随机变量 $Y$ 的取值为 0或1。

对应 **二项**，还有 **多项**，指随机变量 $Y$ 的取值不仅仅只有两个，后面也会进行介绍。

### 模型定义

二项逻辑回归模型假设预测值 $Y$ 在随机变量 $X$ 给定的条件下，条件概率满足下面的分布：

$$
P(Y=1|x)=\frac{\exp(w \cdot x) + b}{1+\exp(w \cdot x +b)}\\
P(Y=0|x)=\frac{1}{1+\exp(w \cdot x +b)}
$$

其中，$x \in R^n$ ，$Y \in {0,1}$ 是输出，
$w \in R^n$ 和 $b \in R$ 是模型参数，
$w$ 是权值向量，$b$ 是偏置，$w \cdot x$ 表示 $w$ 和 $b$ 的内积。

给定一个实例 $x$ ，利用上面两个公式可以计算得到 $P(Y=1|x)$ 和 $P(Y=0|x)$ ，
逻辑回归会比较两个条件概率值的大小，将实例 $x$ 分到概率值较大的类别中。

> 为了处理方便，可以将权值向量和输入向量进行扩充，仍记作 $w$ 和 $x$ ，不过对二者的结构进行调整，
> 令 $w=(w^{(1)}, w^{(2)}, \dots, w^{(n)}, b)^T$ ，$x=(x^{(1)},x^{(2)}, \dots, x^{(n)}, 1)^T$ ，
> 如此处理可以忽略对偏置 $b$ 的处理，相应的，逻辑回归模型可以简化为：
> $$
> P(Y=1|x)=\frac{\exp(w \cdot x) }{1+\exp(w \cdot x )} \\
> P(Y=0|x)=\frac{1}{1+\exp(w \cdot x )}
> $$

**为了方便，后续的说明中都会采用这种扩充形式。**

下面从另一个角度对逻辑回归进行理解。

首先介绍一下 **几率（odds）** 的定义，几率是针对一个事件而言，
它是指该事件发生的概率与该事件不发生的概率的比值。即，若一个时间发生的概率为 $p$ ，
则该事件的几率为 $\frac{p}{1-p}$ ，进一步地，有该时间的对数几率（log odds）或者说 logit 函数为

$$
logit(p) = \log \frac{p}{1-p}
$$

根据对数几率的定义，可以求出，对于一个实例 $x$ 逻辑模型的对数几率为

$$
\log \frac{P(Y=1|x)}{1-P(Y=1|x)}=w \cdot x
$$

从该公式可以看出，在逻辑回归模型中，给一个实例 $x$ ，它的预测结果为1（即 Y=1 ）的对数几率是输入 $x$ 的线性函数。

另一种理解是，考虑 $w \cdot x$ 是对 $x$ 进行分类的线性函数，
逻辑回归模型可以将这个线性函数转化为 $(Y=1|x)$ 概率

$$
P(Y=1|x)=\frac{\exp(w \cdot x) }{1+\exp(w \cdot x )}
$$

此时，当线性函数的值越大，$P(Y=1|x)$ 越大，越接近于1，相反，则越接近于0.

### 参数估计

根据前面的定义，可以发现该模型的参数由两个，分别是权值向量 $w$ 和偏置 $b$ （考虑扩充的话，只有 $w$ ），
在给定训练数据 $T={(x_1, y_1),(x_2, y_2), \dots , (x_N, y_N)}$ ，则可以通过 **极大似然估计** 去估计模型参数。

具体求解过程如下：

> 设
> $$
> P(Y=1|x) = \pi(x), P(Y=0|x)=1-\pi(x)
> $$
>
> 则可计算似然函数为
>
> $$
> \prod^N_{i=1}[\pi(x_i)]^{y_i} [1-\pi(x_i)]^{1-y_i}
> $$
> 由于 $y_i \in {0, 1}$ ，故似然函数累乘项的每个子项前后两部分必由一部分的取值为1
> 对上式取对数，可得对数似然函数为
>
> $$
> \begin{aligned}
> L(w) &= \sum^N_{i=1}[y_i\log \pi(x_i)+(1-y_i)\log(1-\pi(x_i))] \\
> &= \sum^N_{i=1}[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))] \\
> &= \sum^N_{i=1}[y_i(w \cdot x_i) - \log (1+\exp(w \cdot x_i))]
> \end{aligned}
> $$
>
> 利用梯度下降法或拟牛顿法等求解使 $L(w)$ 最大的参数 $w$ 便可以求得模型参数估计值 $\hat w$ .
> 由此可得模型为
> $$
> P(Y=1|x)=\frac{\exp(\hat w \cdot x) }{1+\exp(\hat w \cdot x )} \\
> P(Y=0|x)=\frac{1}{1+\exp(\hat w \cdot x )}
> $$

## 多项逻辑斯蒂回归

将二分类模型推广到多项分类，便得到多项逻辑斯蒂回归模型。模型定义如下：

假设离散随机变量 $Y$ 的取值集合是 $\{1, 2, \dots, K\}$ ，
那么多项逻辑斯蒂回归模型是

$$ \begin{aligned}
P(Y=k|x)&=\frac{\exp(w_k \cdot x)}{1+\sum^{K-1}_{k=1}\exp(w_k \cdot x)}, k=1,2, \dots, K-1 \\
P(Y=K|x)&=\frac{1}{1+\sum^{K-1}_{k=1}\exp(w_k \cdot x)}
\end{aligned}
$$

其中，$x \in R^{n+1}, w_k \in R^{n+1}$ 。

多项逻辑斯蒂回归模型的参数估计方法与二项逻辑斯蒂回归模型的参数估计方法类似。
